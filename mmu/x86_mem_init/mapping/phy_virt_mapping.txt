
page_address() will get the virtual address which corresponds to the 'struct
page *' for the kernel direct memory range.


1. void *page_address(const struct page *page)

There are three definitions in include/linux/mm.h;

1)
#if defined(WANT_PAGE_VIRTUAL)
static inline void *page_address(const struct page *page)
{
	return page->virtual;
}
...
#endif

This is special for several arches.

2)
#if defined(HASHED_PAGE_VIRTUAL)
void *page_address(const struct page *page);

For this case, the real definition is in mm/highmem.c;
specific for x86_32; For X86_64, there is no HIGHMEM.

3)

#if !defined(HASHED_PAGE_VIRTUAL) && !defined(WANT_PAGE_VIRTUAL)
#define page_address(page) lowmem_page_address(page)

This is for X86_64, etc.


static __always_inline void *lowmem_page_address(const struct page *page)
{
	return page_to_virt(page);
}



So, we concentrate on the 'page_to_virt()';

There are two definitions of 'page_to_virt' :

1) include/linux/mm.h

#ifndef page_to_virt
#define page_to_virt(x)	__va(PFN_PHYS(page_to_pfn(x)))
#endif

2) include/asm-generic/page.h
#define page_to_virt(page)	pfn_to_virt(page_to_pfn(page))


So, which one is for X86_64 or ARM64?

yzc@yzc-linaro:~/linaro/work_dir/its_v3/src$ grep -nrI 'asm-generic\/page.h' .
--include=*.h --include=*.c
./arch/h8300/include/asm/page.h:4:#include <asm-generic/page.h>
./arch/c6x/include/asm/page.h:9:#include <asm-generic/page.h>
./arch/blackfin/include/asm/page.h:18:#include <asm-generic/page.h>

It is obvious that the 2nd definition are not suitable for X86_64 or ARM64.



Q1. what is the role of page_to_pfn(x)?

Here, x is the virtual addr of 'struct page';

You can refer to the include/asm-generic/memory_model.h :

We only care about the mechanism based on 'CONFIG_SPARSEMEM_VMEMMAP':

#elif defined(CONFIG_SPARSEMEM_VMEMMAP)

/* memmap is virtually contiguous.  */
#define __pfn_to_page(pfn)	(vmemmap + (pfn))
#define __page_to_pfn(page)	(unsigned long)((page) - vmemmap)


Then PFN_PHYS(page_to_pfn(x)) can work based on this :

#define PFN_PHYS(x)	((phys_addr_t)(x) << PAGE_SHIFT)

Q2. what is _va() and _pa()?

in arch/x86/include/asm/page.h :

#ifndef __pa
#define __pa(x)		__phys_addr((unsigned long)(x))
#endif

#ifndef __va
#define __va(x)			((void *)((unsigned long)(x)+PAGE_OFFSET))
#endif

There are two definition of __phys_addr for each X86 architecture.
we only focus on the definition in arch/x86/mm/physaddr.c for
CONFIG_DEBUG_VIRTUAL:

unsigned long __phys_addr(unsigned long x)
{
	unsigned long y = x - __START_KERNEL_map;

	/* use the carry flag to determine if x was < __START_KERNEL_map */
	if (unlikely(x > y)) {
		x = y + phys_base;

		VIRTUAL_BUG_ON(y >= KERNEL_IMAGE_SIZE);
	} else {
		x = y + (__START_KERNEL_map - PAGE_OFFSET);

		/* carry flag will be set if starting x was >= PAGE_OFFSET */
		VIRTUAL_BUG_ON((x > y) || !phys_addr_valid(x));
	}

	return x;
}


We know for x86_64, the input virtual 'x' should be bigger than PAGE_OFFSET, or
over than __START_KERNEL_map specific for 'direct mapping of all phys. memory';




