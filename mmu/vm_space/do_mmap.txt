
unsigned long do_mmap(struct file *file, unsigned long addr,
			unsigned long len, unsigned long prot,
			unsigned long flags, vm_flags_t vm_flags,
			unsigned long pgoff, unsigned long *populate,
			struct list_head *uf)


1. get_unmapped_area(file, addr, len, pgoff, flags)

	addr = get_unmapped_area(file, addr, len, pgoff, flags);
	if (offset_in_page(addr))
		return addr;



1.1 If the 'MAP_FIXED' is enabled in flags

the designated parameter 'addr' will not be modified and return directly with
the original value.
More important is that:

       MAP_FIXED
              Don't  interpret  addr  as  a hint: place the mapping at exactly
              that address.  addr must be a multiple of the page size.  If the
              memory  region  specified  by addr and len overlaps pages of any
              existing mapping(s), then the overlapped part  of  the  existing
              mapping(s)  will  be discarded. 

So, there is edge effect with MAP_FIXED.
And we can understand these code in mmap_region():

	while (find_vma_links(mm, addr, addr + len, &prev, &rb_link,
			      &rb_parent)) {
		if (do_munmap(mm, addr, len, uf))
			return -ENOMEM;
	}

Normally, if MAP_FIXED is disabled, the below code will select available empty
VM sub-region for this mmaping. Kernel will try its best to get unmapped area
which match the input 'addr, addr + len', but if there are existing mapped areas
overlapped with this requested area, kernel will allocate other available area :

	if (addr) {
		addr = PAGE_ALIGN(addr);
		vma = find_vma_prev(mm, addr, &prev);
		if (TASK_SIZE - len >= addr && addr >= mmap_min_addr &&
		    (!vma || addr + len <= vm_start_gap(vma)) &&
		    (!prev || addr >= vm_end_gap(prev)))
			return addr;
	}

	info.flags = 0;
	info.length = len;
	info.low_limit = mm->mmap_base;
	info.high_limit = TASK_SIZE;
	info.align_mask = 0;
	return vm_unmapped_area(&info);

(The above code is in arch_get_unmapped_area() from mm/mmap.c)

So, for non-MAP-FIXED mmap, the do_munmap() handling should not happen. It is
only for MAP-FIXED mmaping, I think.

1.2. arch_get_unmapped_area() of mm/mmap.c is suitable for X86_32

There are specific arch_get_unmapped_area() in arch/x86/kernel/sys_x86_64.c for
X86_64. It seems the major differences lie in these code:

	if (filp) {
		info.align_mask = get_align_mask();
		info.align_offset += get_align_bits();
	}

And I think these code is not perfect like the one in mm/mmap.c :
	if (addr) {
		addr = PAGE_ALIGN(addr);
		vma = find_vma(mm, addr);
		if (end - len >= addr &&
		    (!vma || addr + len <= vm_start_gap(vma)))
			return addr;
	}

It is probably the return 'addr, addr + len' will overlap with the 'prev'
sub-range.

2. The access/protection attribute flags

	vm_flags |= calc_vm_prot_bits(prot, pkey) | calc_vm_flag_bits(flags) |
			mm->def_flags | VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC;

The above processing will convert the original input 'flags' or 'prot' to the
corresponding kernel VM flags. The core is macro _calc_vm_trans(x, bit1, bit2).
We take this as example:
	_calc_vm_trans(prot, PROT_READ,  VM_READ )

If the PROT_READ bit of 'prot' is set, then this macro will return a value whose
VM_READ bit is set(only this bit).


2.1 

Q1? What are the relationship between vm_flags, file->f_mode, inode->i_flags?



Q2? What is the difference between VM_MAYWRITE, VM_WRITE

2.2 'vma->vm_page_prot' vs 'vma->vm_flags'


pgprot_t protection_map[16] = {
	__P000, __P001, __P010, __P011, __P100, __P101, __P110, __P111,
	__S000, __S001, __S010, __S011, __S100, __S101, __S110, __S111
};

pgprot_t vm_get_page_prot(unsigned long vm_flags)
{
	return __pgprot(pgprot_val(protection_map[vm_flags &
				(VM_READ|VM_WRITE|VM_EXEC|VM_SHARED)]) |
			pgprot_val(arch_vm_get_page_prot(vm_flags)));
}

The array index of protection_map[16] is decided by the bit value of
'(VM_READ|VM_WRITE|VM_EXEC|VM_SHARED)' in 'vm_flags'. The bit 0 is VM_READ, bit
1 is VM_WRITE, bit 2 is VM_EXEC, bit 3 is VM_SHARED. So, total element count is
16.

With the values defined in 'protection_map[16]', kernel can convert the lowest 4
bits of vm_flags into the page attributes defined in
arch/x86/asm/pgtable_types.h, such as :

#define PAGE_SHARED	__pgprot(_PAGE_PRESENT | _PAGE_RW | _PAGE_USER | \
				 _PAGE_ACCESSED | _PAGE_NX)

#define PAGE_COPY_NOEXEC	__pgprot(_PAGE_PRESENT | _PAGE_USER |	\
					 _PAGE_ACCESSED | _PAGE_NX)
...
#define PAGE_COPY		PAGE_COPY_NOEXEC


On x86,
	if VM_WRITE is set, VM_READ is implicitly set too, which means the page
	attributes between __P010 and __P011, __P110 and __P111 are same.

	if VM_SHARED is set, _PAGE_RW will be set in the PTE;

It seems the PAGE_COPY is for COW when a specific private page data is needed,
then the copy from the original page can be performed.






